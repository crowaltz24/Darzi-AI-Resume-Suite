#!/usr/bin/env python3
"""
FastAPI backend with integrated MCP for resume parsing
Combines local parser with AI-enhanced MCP parsing
"""

import uvicorn
import tempfile
import os
import json
from typing import Optional, Dict, Any, List, Union
from fastapi import FastAPI, Request, HTTPException, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
from fastmcp import Client

from api.utils.parser import ResumeParser

# Configuration
#MCP_SERVER_URL = "stdio://python server.py"  # Local MCP server
# MCP_SERVER_URL = "https://your-remote-mcp-server.com/mcp/"  # Remote option
import os
from pathlib import Path

BASE_DIR = Path(__file__).parent.parent
MCP_SERVER_PATH = BASE_DIR / "local_mcp" / "server.py"
MCP_SERVER_URL = f"stdio://python {MCP_SERVER_PATH}"

#global variables
client: Optional[Client] = None
resume_parser: Optional[ResumeParser] = None

#standard JSON schema for all parsers
RESUME_SCHEMA = {
    "name": "",
    "email": [],
    "mobile_number": [],
    "skills": [],
    "education": [],
    "experience": [],
    "summary": "",
    "certifications": [],
    "raw_text": "",
    "parsing_source": "",
    "confidence_score": 0.0
}

@asynccontextmanager
async def lifespan(app: FastAPI):
    global client, resume_parser
    
    print("Starting Darzi Resume Parser API...") #starter
    
    #initialize local parser
    try:
        resume_parser = ResumeParser()
        print("Local parser initialized")
    except Exception as e:
        print(f"Local parser failed: {e}")
        resume_parser = None
    
    #initialize MCP client
    try:
        client = Client(MCP_SERVER_URL)
        await client.__aenter__()
        print("MCP client connected")
        
        #test MCP connection
        tools = await client.list_tools()
        print(f"Available MCP tools: {[tool['name'] for tool in tools]}")
        
    except Exception as e:
        print(f"MCP client connection failed: {e}")
        client = None
    
    yield
    
    #shutdown
    print("Shutting down...")
    if client:
        try:
            await client.__aexit__(None, None, None)
            print("MCP client disconnected")
        except Exception as e:
            print(f"Error during MCP shutdown: {e}")
    
    print("Shutdown complete!")


app = FastAPI(
    title="Darzi Resume Parser & ATS Optimizer",
    description="AI-powered resume parsing with ATS optimization",
    version="2.0.0",
    lifespan=lifespan
)

#CORS middleware
origins = [
    "http://localhost:3000",
    "http://localhost:8000", 
    "http://127.0.0.1:3000",
    "http://127.0.0.1:8000",
    #we can add out production domain (darzi-demo.vercel.app)
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["*"],  
)

def normalize_to_schema(data: Dict[str, Any], source: str = "unknown") -> Dict[str, Any]:
    normalized = RESUME_SCHEMA.copy()
    
    for key, default_value in RESUME_SCHEMA.items():
        if key in data:
            value = data[key]
            
            if key in ['email', 'mobile_number', 'skills', 'education', 'experience', 'certifications']:
                if isinstance(value, list):
                    normalized[key] = value
                elif isinstance(value, str) and value.strip():
                    normalized[key] = [value.strip()]
                else:
                    normalized[key] = []
            
            elif key in ['name', 'summary', 'raw_text', 'parsing_source']:
                normalized[key] = str(value) if value else ""
            
            elif key == 'confidence_score':
                try:
                    normalized[key] = float(value) if value else 0.0
                except (ValueError, TypeError):
                    normalized[key] = 0.0
            
            else:
                normalized[key] = value
    
    normalized['parsing_source'] = source
    return normalized

async def parse_with_local(text: str) -> Dict[str, Any]:            #parser esume using loccal parser
    if not resume_parser:
        raise HTTPException(status_code=500, detail="Local parser not available")
    
    try:
        result = {
            "name": resume_parser.extract_name(text),
            "email": resume_parser.extract_email(text),
            "mobile_number": resume_parser.extract_phone(text),
            "skills": resume_parser.extract_skills(text),
            "education": resume_parser.extract_education(text),
            "experience": resume_parser.extract_experience(text),
            "summary": "",  #local parser doesn't extract summary rn
            "certifications": [],  #local parser doesn't extract certifications too
            "raw_text": text[:500] + "..." if len(text) > 500 else text,
            "confidence_score": 0.7  #default confidence for local parser (added for later llm integration)
        }
        
        return normalize_to_schema(result, "local")
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Local parsing failed: {str(e)}")

async def parse_with_mcp(text: str) -> Dict[str, Any]:      #Parse resume using MCP AI service
    if not client:
        raise HTTPException(status_code=503, detail="MCP service not available")
    
    try:
        result = await client.call_tool("parse_resume", {"text": text})
        return normalize_to_schema(result, "mcp")
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"MCP parsing failed: {str(e)}")

def merge_parsing_results(local_result: Dict[str, Any], mcp_result: Dict[str, Any]) -> Dict[str, Any]:
    merged = RESUME_SCHEMA.copy()
    
    #use MCP name if available, otherwise local
    merged['name'] = mcp_result.get('name') or local_result.get('name', '')
    
    #merge email (prefer non-empty results)
    merged['email'] = list(set(
        (mcp_result.get('email', []) or []) + 
        (local_result.get('email', []) or [])
    ))
    
    merged['mobile_number'] = list(set(
        (mcp_result.get('mobile_number', []) or []) + 
        (local_result.get('mobile_number', []) or [])
    ))
    
    merged['skills'] = list(set(
        (mcp_result.get('skills', []) or []) + 
        (local_result.get('skills', []) or [])
    ))
    
    merged['education'] = mcp_result.get('education', []) or local_result.get('education', [])
    
    merged['experience'] = mcp_result.get('experience', []) or local_result.get('experience', [])
    
    merged['summary'] = mcp_result.get('summary', '')
    merged['certifications'] = mcp_result.get('certifications', [])
    
    #using local raw text
    merged['raw_text'] = local_result.get('raw_text', '')
    
    local_conf = local_result.get('confidence_score', 0.7)
    mcp_conf = mcp_result.get('confidence_score', 0.8)
    merged['confidence_score'] = (local_conf + mcp_conf) / 2
    
    merged['parsing_source'] = 'hybrid'
    
    return merged


@app.get("/")
async def root():
    return {
        "message": "Darzi Resume Parser & ATS Optimizer API",
        "version": "2.0.0",
        "status": "running",
        "services": {
            "local_parser": "available" if resume_parser else "unavailable",
            "mcp_ai_parser": "available" if client else "unavailable"
        },
        "endpoints": {
            "health": "/health",
            "parse_text": "/parse",
            "parse_pdf": "/parse-pdf",
            "mcp_status": "/mcp-status",
            "docs": "/docs"
        }
    }

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "timestamp": "2024-01-01T00:00:00Z",
        "components": {
            "api": "healthy",
            "local_parser": "healthy" if resume_parser else "unhealthy",
            "mcp_client": "healthy" if client else "unhealthy"
        },
        "capabilities": {
            "text_parsing": bool(resume_parser or client),
            "pdf_parsing": bool(resume_parser),
            "ai_enhancement": bool(client),
            "hybrid_parsing": bool(resume_parser and client)
        }
    }

@app.get("/mcp-status")     #for checking MCP service status and available tools
async def mcp_status():
    if not client:
        return {
            "status": "disconnected",
            "message": "MCP client not available",
            "tools": []
        }
    
    try:
        tools = await client.list_tools()
        return {
            "status": "connected",
            "message": "MCP client connected successfully",
            "tools": [
                {
                    "name": tool.get("name", ""),
                    "description": tool.get("description", "")
                }
                for tool in tools
            ]
        }
    except Exception as e:
        return {
            "status": "error",
            "message": f"MCP client error: {str(e)}",
            "tools": []
        }

@app.post("/parse")
async def parse_resume_text(request: Request):
    try:
        # Read request body
        body_bytes = await request.body()
        text = body_bytes.decode("utf-8").strip()
        
        if not text:
            raise HTTPException(status_code=400, detail="Empty request body")
        
        if len(text) < 50:
            raise HTTPException(status_code=400, detail="Text too short to be a valid resume")
        
        local_available = bool(resume_parser)
        mcp_available = bool(client)
        
        if not local_available and not mcp_available:
            raise HTTPException(status_code=503, detail="No parsing services available")
        
        local_result = None
        mcp_result = None
        errors = {}
        
        if local_available:
            try:
                local_result = await parse_with_local(text)
            except Exception as e:
                errors['local_parser'] = str(e)
                print(f"Local parsing failed: {e}")
        
        if mcp_available:
            try:
                mcp_result = await parse_with_mcp(text)
            except Exception as e:
                errors['mcp_parser'] = str(e)
                print(f"MCP parsing failed: {e}")
        
        if local_result and mcp_result:
            final_result = merge_parsing_results(local_result, mcp_result)            #hybrid approach -> merge results
        elif local_result:
            final_result = local_result
        elif mcp_result:
            final_result = mcp_result
        else:
            raise HTTPException(status_code=500, detail="All parsing methods failed")
        
        response = {
            "success": True,
            "data": final_result,
            "metadata": {
                "text_length": len(text),
                "parsing_method": final_result['parsing_source'],
                "confidence": final_result['confidence_score']
            }
        }
        
        if errors:
            response['warnings'] = errors
        
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Parsing failed: {str(e)}")

@app.post("/parse-pdf")
async def parse_resume_pdf(file: UploadFile = File(...)):       #Parse resume from PDF file Currently uses local parser only (MCP doesn't handle files directly)

    if not resume_parser:
        raise HTTPException(status_code=503, detail="PDF parsing service not available")

    if not file.filename:
        raise HTTPException(status_code=400, detail="No filename provided")
        
    if not file.filename.lower().endswith('.pdf'):
        raise HTTPException(status_code=400, detail="Only PDF files are supported")
    
    MAX_SIZE = 10 * 1024 * 1024     #kept file size to 10mb
    
    tmp_path = None
    try:
        content = await file.read()
        
        if len(content) == 0:
            raise HTTPException(status_code=400, detail="Empty file")
        
        if len(content) > MAX_SIZE:
            raise HTTPException(status_code=400, detail="File too large (max 10MB)")

        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
            tmp_file.write(content)
            tmp_path = tmp_file.name
        
        result = resume_parser.parse_resume(tmp_path)
        
        if "error" in result:
            raise HTTPException(status_code=400, detail=result["error"])
        
        normalized_result = normalize_to_schema(result, "local_pdf")
        
        return {
            "success": True,
            "data": normalized_result,
            "metadata": {
                "filename": file.filename,
                "file_size": len(content),
                "parsing_method": "local_pdf",
                "confidence": normalized_result['confidence_score']
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"PDF parsing failed: {str(e)}")
    
    finally:
        if tmp_path and os.path.exists(tmp_path):
            try:
                os.unlink(tmp_path)
            except Exception as cleanup_error:
                print(f"Warning: Could not cleanup {tmp_path}: {cleanup_error}")

@app.post("/optimize-ats")
async def optimize_for_ats(request: Request):
    try:
        body = await request.json()
        resume_text = body.get('resume_text', '')
        job_description = body.get('job_description', '')
        
        if not resume_text:
            raise HTTPException(status_code=400, detail="resume_text is required")
        
        try:
            if resume_parser and client:
                local_result = await parse_with_local(resume_text)
                mcp_result = await parse_with_mcp(resume_text)
                parsed_resume = merge_parsing_results(local_result, mcp_result)
            elif client:
                parsed_resume = await parse_with_mcp(resume_text)
            elif resume_parser:
                parsed_resume = await parse_with_local(resume_text)
            else:
                raise HTTPException(status_code=503, detail="No parsing service available")
        
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Failed to parse resume: {str(e)}")
        
        ats_suggestions = analyze_ats_compatibility(parsed_resume, job_description)
        
        return {
            "success": True,
            "data": {
                "parsed_resume": parsed_resume,
                "ats_score": ats_suggestions["score"],
                "suggestions": ats_suggestions["suggestions"],
                "optimized_sections": ats_suggestions["optimized_sections"]
            },
            "metadata": {
                "optimization_method": "rule_based",
                "job_description_provided": bool(job_description)
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ATS optimization failed: {str(e)}")

def analyze_ats_compatibility(resume_data: Dict[str, Any], job_description: str = "") -> Dict[str, Any]:
    score = 0
    max_score = 100
    suggestions = []
    optimized_sections = {}
    
    #check basic information completeness (20 points)
    if resume_data.get('name'):
        score += 5
    else:
        suggestions.append("Add your full name at the top of the resume")
    
    if resume_data.get('email'):
        score += 5
    else:
        suggestions.append("Include a professional email address")
    
    if resume_data.get('mobile_number'):
        score += 5
    else:
        suggestions.append("Add your phone number")
    
    if resume_data.get('skills'):
        score += 5
    else:
        suggestions.append("Include a skills section with relevant technical and soft skills")
    
    #check skills section quality (25 points)
    skills = resume_data.get('skills', [])
    if len(skills) >= 5:
        score += 10
        if len(skills) >= 10:
            score += 5
    else:
        suggestions.append("Add more skills (aim for 10-15 relevant skills)")
    
    #check for technical skills
    technical_keywords = ['python', 'java', 'javascript', 'sql', 'html', 'css', 'react', 'node', 'aws', 'docker']
    tech_skills_found = [skill for skill in skills if any(tech.lower() in skill.lower() for tech in technical_keywords)]
    if tech_skills_found:
        score += 10
    else:
        suggestions.append("Include more technical skills relevant to your field")
    
    #check experience section (25 points)
    experience = resume_data.get('experience', [])
    if experience:
        score += 10
        if len(experience) >= 2:
            score += 5
        
        #check for quantified achievements
        exp_text = str(experience).lower()
        if any(indicator in exp_text for indicator in ['%', ' '
        , 'increased', 'decreased', 'improved', 'reduced']):
            score += 10
        else:
            suggestions.append("Add quantified achievements to your experience (e.g., 'Increased efficiency by 20%')")
    else:
        suggestions.append("Include work experience with specific achievements")
    
    #check education section (15 points)
    education = resume_data.get('education', [])
    if education:
        score += 15
    else:
        suggestions.append("Add your educational background")
    
    #check summary/objective (15 points)
    summary = resume_data.get('summary', '')
    if summary and len(summary) > 50:
        score += 15
    else:
        suggestions.append("Add a professional summary (2-3 sentences highlighting your key strengths)")
    
    #job description keyword matching (if provided)
    if job_description:
        keyword_analysis = analyze_keywords_match(resume_data, job_description)
        optimized_sections['keywords'] = keyword_analysis
        
        if keyword_analysis['match_percentage'] >= 70:
            score += 0  #already good ig
        elif keyword_analysis['match_percentage'] >= 50:
            suggestions.append(f"Consider adding these missing keywords: {', '.join(keyword_analysis['missing_keywords'][:5])}")
        else:
            suggestions.append("Significantly improve keyword matching with the job description")
    
    #ATS-friendly formatting suggestions
    formatting_suggestions = []
    
    resume_text_lower = resume_data.get('raw_text', '').lower()
    standard_sections = ['experience', 'education', 'skills', 'summary']
    missing_sections = [section for section in standard_sections if section not in resume_text_lower]
    
    if missing_sections:
        formatting_suggestions.append(f"Use standard section headers: {', '.join(missing_sections)}")
    
    formatting_suggestions.extend([
        "Use simple, clean formatting without graphics or unusual fonts",
        "Avoid tables, text boxes, and columns",
        "Use bullet points for achievements and responsibilities",
        "Save as PDF to preserve formatting",
        "Use standard fonts like Arial, Calibri, or Times New Roman"
    ])
    
    optimized_sections['formatting'] = formatting_suggestions
    
    final_score = min(score, max_score)
    
    #score based sugesstion
    if final_score < 50:
        suggestions.insert(0, "Your resume needs significant improvement for ATS compatibility")
    elif final_score < 70:
        suggestions.insert(0, "Your resume is moderately ATS-friendly but has room for improvement")
    elif final_score < 85:
        suggestions.insert(0, "Your resume is well-optimized for ATS with minor improvements needed")
    else:
        suggestions.insert(0, "Excellent! Your resume is highly optimized for ATS systems")
    
    return {
        "score": final_score,
        "max_score": max_score,
        "suggestions": suggestions[:10],  # Limit to top 10 suggestions
        "optimized_sections": optimized_sections
    }

def analyze_keywords_match(resume_data: Dict[str, Any], job_description: str) -> Dict[str, Any]:
    job_keywords = extract_job_keywords(job_description)
    
    resume_text = ' '.join([
        resume_data.get('summary', ''),
        ' '.join(resume_data.get('skills', [])),
        str(resume_data.get('experience', [])),
        str(resume_data.get('education', []))
    ]).lower()
    
    matching_keywords = []
    missing_keywords = []
    
    for keyword in job_keywords:
        if keyword.lower() in resume_text:
            matching_keywords.append(keyword)
        else:
            missing_keywords.append(keyword)
    
    match_percentage = (len(matching_keywords) / len(job_keywords) * 100) if job_keywords else 0
    
    return {
        "job_keywords": job_keywords,
        "matching_keywords": matching_keywords,
        "missing_keywords": missing_keywords,
        "match_percentage": round(match_percentage, 1),
        "total_keywords": len(job_keywords)
    }

def extract_job_keywords(job_description: str) -> List[str]:          
    keyword_patterns = [
        #programming languages
        'python', 'java', 'javascript', 'typescript', 'c++', 'c#', 'php', 'ruby', 'go', 'rust',
        #web techn..
        'html', 'css', 'react', 'angular', 'vue', 'node.js', 'express', 'django', 'flask',
        #databases
        'sql', 'mysql', 'postgresql', 'mongodb', 'redis', 'sqlite',
        #cloud & DevOps
        'aws', 'azure', 'gcp', 'docker', 'kubernetes', 'jenkins', 'terraform', 'ansible',
        #data & AI
        'machine learning', 'data science', 'tensorflow', 'pytorch', 'pandas', 'numpy',
        #tools
        'git', 'jira', 'confluence', 'agile', 'scrum', 'ci/cd',
        #soft skills
        'leadership', 'communication', 'teamwork', 'problem solving', 'analytical'
    ]
    
    found_keywords = []
    job_text_lower = job_description.lower()
    
    for keyword in keyword_patterns:
        if keyword in job_text_lower:
            found_keywords.append(keyword)
    
    import re
    capitalized_words = re.findall(r'\b[A-Z][a-zA-Z]+\b', job_description)
    technical_caps = [word for word in capitalized_words if len(word) > 3 and word not in ['The', 'And', 'For', 'With']]
    
    found_keywords.extend(technical_caps[:10])  #limit to 10 additional terms
    
    return list(set(found_keywords))  #fro removing dubpilcates

if __name__ == "__main__":
    uvicorn.run(
        "main:app", 
        host="0.0.0.0", 
        port=7860, 
        reload=True,
        log_level="info"
    )